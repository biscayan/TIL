# End-to-end accented speech recognition
## 공통
model, parameter등은 동일하게 하고, dataset을 바꿔가며 실험들을 진행하였다.  
실험의 세팅은 다음과 같다.  

- model: 1 layers of Resnet (CNN) + 2 layers of BiGRU (RNN)
- feature: 128 mel filterbanks of mel spectrogram
- data augmentation: spec augmentation with frequency masking and time masking
- RNN hidden dimension: 256
- dropout rate: 0.2
- learning rate: 0.0001
- batch size: 32
- loss function: CTC loss
- optimizer: AdamW
- number of classes: 29 (A-Z, apostrophe, space, blank)
- edit distance: levenshtein distance
- decoder: greedy decoder

## 실험1
- commonvoice
- random split
- accent class: 1 (US)
- dataset (8:1:1)  
(1) train set: 80000 files (99 hours)  
(2) validation set: 10000 files (12 hours)  
(3) test set: 10000 files (12 hours)  
- epoch: 100
- test loss: 1.214157
- CER: 35.69
- WER: 81.04%

## 실험2
- commonvoice
- model: 3 layers of Resnet (CNN) + 5 layers of BiGRU (RNN)
- accent class: 5 (Australia, Canada, England, India, US)
- random split
- dataset (8:1:1)  
(1) train set: 80000 files (101 hours)  
(2) validation set: 10000 files (14 hours)  
(3) test set: 10000 files (13 hours)  
- epoch: 100
- test loss: 1.203911
- CER: 35.14%
- WER: 79.43%

## 실험3
- librispeech
- dataset  
(1) train set: 28539 files (101 hours)  
(2) validation set: 2703 files (5 hours)  
(3) test set: 2620 files (5 hours)  
- epoch: 100
- test loss:
- CER: 
- WER: 

## TODO

