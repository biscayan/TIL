# End-to-end accented speech recognition
## 공통
model, parameter등은 동일하게 하고, dataset을 바꿔가며 실험들을 진행하였다.  
실험의 세팅은 다음과 같다.  

- model: 3 layers of Resnet (CNN) + 3 layers of BiGRU (RNN)
- feature: 128 mel filterbanks of mel spectrogram
- data augmentation: spec augmentation with frequency masking and time masking
- RNN hidden dimension: 512
- dropout rate: 0.1
- learning rate: 0.0001
- batch size: 32
- epoch: 100
- loss function: CTC loss
- optimizer: AdamW
- number of classes: 29 (A-Z, apostrophe, space, blank)
- edit distance: levenshtein distance
- decoder: greedy decoder

## 실험1
- librispeech
- dataset  
(1) train set: 28539 files (101 hours)  
(2) validation set: 2703 files (5 hours)  
(3) test set: 2620 files (5 hours)  
- test loss: 0.462640
- CER: 11.97%
- WER: 36.91%

## 실험2
- commonvoice
- accent class: 5 (Australia, Canada, England, India, US)
- random split
- dataset (8:1:1)  
(1) train set: 80000 files (101 hours)  
(2) validation set: 10000 files (14 hours)  
(3) test set: 10000 files (13 hours)  
- test loss: 0.873942
- CER: 25.60%
- WER: 64.15%

## 실험3
- commonvoice
- random split
- accent class: 1 (US)
- dataset (8:1:1)  
(1) train set: 80000 files (99 hours)  
(2) validation set: 10000 files (12 hours)  
(3) test set: 10000 files (12 hours)  
- test loss: 0.706712
- CER: 20.60%
- WER: 55.99%

## 실험4
- librispeech
- RNN hidden dimension: 1024
- dataset  
(1) train set: 28539 files (101 hours)  
(2) validation set: 2703 files (5 hours)  
(3) test set: 2620 files (5 hours)  
- test loss: 0.542798
- CER: 10.65%
- WER: 32.88%