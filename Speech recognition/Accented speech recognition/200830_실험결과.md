# End-to-end accented speech recognition
## 공통
model, parameter등은 동일하게 하고, dataset을 바꿔가며 실험들을 진행하였다.  
실험의 세팅은 다음과 같다.  

- model: 1 layers of Resnet (CNN) + 2 layers of BiGRU (RNN)
- feature: 128 mel filterbanks of mel spectrogram
- data augmentation: spec augmentation with frequency masking and time masking
- RNN hidden dimension: 256
- dropout rate: 0.2
- learning rate: 0.0001
- batch size: 32
- loss function: CTC loss
- optimizer: AdamW
- number of classes: 29 (A-Z, apostrophe, space, blank)
- edit distance: levenshtein distance
- decoder: greedy decoder

## 실험1
- commonvoice
- accent class: 5 (Australia, Canada, England, India, US)
- dataset (6:2:2)  
(1) train set: 15000 files (15 hours)  
(2) validation set: 5000 files (5 hours)  
(3) test set: 5000 files (5 hours)  
- epoch: 150
- test loss: 1.675894
- CER: 46.97%
- WER: 91.70%

## 실험2-1
- commonvoice
- accent class: 5 (Australia, Canada, England, India, US)
- dataset (6:2:2)  
(1) train set: 30000 files (31 hours)  
(2) validation set: 10000 files (10 hours)  
(3) test set: 10000 files (10 hours)  
- epoch: 150
- test loss: 1.443637
- CER: 42.14%
- WER: 87.24%

## 실험2-2
- commonvoice
- accent class: 5 (Australia, Canada, England, India, US)
- dataset (6:2:2)  
(1) train set: 30000 files (31 hours)  
(2) validation set: 10000 files (10 hours)  
(3) test set: 10000 files (10 hours)  
- epoch: 200
- test loss: 1.421680
- CER: 41.29%
- WER: 85.20%

## 실험2-3
- commonvoice
- accent class: 4 (Australia, Canada, England, US)
- dataset (6:2:2)  
(1) train set: 24000 files (25 hours)  
(2) validation set: 8000 files (8 hours)  
(3) test set: 8000 files (8 hours)  
- epoch: 100
- test loss: 1.430350
- CER: 41.49%  
- WER: 87.23%  

## 실험3-1
- commonvoice
- accent class: 5 (Australia, Canada, England, India, US)
- random split
- dataset (8:1:1)  
(1) train set: 80000 files (101 hours)  
(2) validation set: 10000 files (14 hours)  
(3) test set: 10000 files (13 hours)  
- epoch: 100
- test loss: 1.229171  
- CER: 37.36%  
- WER: 81.79%  

## 실험3-2
- commonvoice
- Resnet -> vanilla CNN
- accent class: 5 (Australia, Canada, England, India, US)
- random split
- dataset (8:1:1)  
(1) train set: 80000 files (101 hours)  
(2) validation set: 10000 files (14 hours)  
(3) test set: 10000 files (13 hours)  
- epoch: 100
- test loss: 2.120900  
- CER: 58.62%  
- WER: 92.13%  

## 번외
- 구글 코랩 환경에서 진행
- librispeech (torchaudio에서 제공)  
- dataset  
(1) train set: 28539 files (100 hours)  
(2) test set: 2620 files  
- epoch: 50
- test loss: 0.867787
- CER: 26.70%
- WER: 70.41%

## TODO
- [ ] 목표: WER을 20%대까지 줄이기 (baseline)
- [x] model 바꿔보기 Resnet -> vanilla CNN, 오히려 성능이 안좋아짐
- [ ] dropout 적용하지 말아보기?
- [ ] epoch을 더 많이? epoch을 더 많이 돌릴수록 성능이 조금씩 좋아지기는 함.
- [ ] dataset size를 더 크게? 하지만, 다른 논문에서는 적은 size로도 괜찮은 성능을 보였음. 단, accent class의 구성은 다름
- [x] model의 layer를 더 깊게, rnn의 hidden dimension을 더 크게 -> cuda memory 부족 에러 발생, 어떻게 해결?
- [ ] learning rate scheduler 사용해보기? 여러가지가 있는데 어떤것을 사용?
- [ ] decoder 바꿔보기 greedy decoder -> beam search decoder
