지금까지의 실험들이 어떻게 진행되었는지를 정리해보고자 글을 쓴다.  
나는 DANN을 이용하여 Accented Speech Recognition의 성능을 높이고자 하는 실험을 진행하고 있다.  
따라서 heavy accent English의 데이터로 Common voice의 Australia, Canada, England, India English총 4개의 corpus를 사용했고,  
standard accent English로는 Librispeech나 Common voice의 US English를 사용하고자 한다.  
앞으로 전자의 데이터들은 target domain data로 칭하고, 후자의 데이터들은 source domain data로 칭하도록 하겠다.  
맨 처음으로 시도했던 실험은, 내가 가지고 있는 데이터를 모두 사용하고자 하는 것이었다.  
source domain data로 120시간 trainset의 Common voice corpus를 사용했을 때는 cuda oom 에러로 실험이 진행되지 않았다.  
반면 100시간 trainset의 librispeech corpus를 사용했을 때는 일부조건만 실험이 완료되었다.  
4개의 target domain의 데이터들은 개수가 모두 달랐고 데이터수가 많은 England English 같은 경우 cuda oom 에러로 실험이 진행되지 않았다.  
따라서 common voice source domain data의 trainset은 100시간으로, target domain data는 25시간으로 통일을 하였다.  
그렇게 데이터를 분류하고 다시 실험을 진행했을 때, librispeech source domain data는 실험이 잘 진행되었지만  
common voice source domain data는 실험이 완벽히 진행되지가 않았다.  
이는 두 corpus가 100시간으로 동일하지만 common voice corpus 파일의 개수가 많음이 cuda oom 에러를 일으키는 것 같았다.  
왜냐하면 librispeech는 대략 30000개 정도의 파일로 구성되어 있지만 common voice는 80000개의 파일로 구성되어 있기 때문이다.  
따라서 common voice corpus를 40000개의 50시간 trainset으로, 그리고 60000개의 75시간의 trainset으로 만들어보았다.  
75시간의 trainset은 여전히 cuda oom 에러가 발생하였다.  
만약 source domain data로 common voice를 사용할거면 50시간의 trainset을 사용해야만 하는 상황이다.  
50시간의 common voice corpus를 가지고 cnn과 rnn의 layer 수를 조정하면서 실험을 진행해봤는데, 
cnn, rnn 모두 3개의 layer가 적당해보였다.  
cnn의 구조가 영향을 끼치는지도 확인하기 위하여 resnet과 vanilla cnn도 비교를 해보았는데,
