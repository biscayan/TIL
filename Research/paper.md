# Paper
The paper list what I read
## Paper list
### Machine Learning & Deep Learning
- A Survey of Deep Active Learning
- A Survey on Deep Transfer Learning
- Active Learning for Convolutional Neural Networks: A Core-Set Approach
- An overview of Multi-Task Learning in Deep Neural Networks
- Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
- Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding
- Distilling the Knowledge in a Neural Network
- Domain-Adversarial Training of Neural Networks
- Gaussian Error Linear Units (GELUs)
- Generative Adversarial Nets
- Gradient Episodic Memory for Continual Learning
- Group Normalization
- Layer Normalization
- Learning Loss for Active Learning
- Learning with Pseudo-Ensembles
- Learning without Forgetting
- Long-short Term Memory
- Monotonic Chunkwise Attention
- Overcoming catastrophic forgetting in neural networks
- Progressive Neural Networks
- Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks
- Representation Learning with Contrastive Predictive Coding
- Searching for Activation Functions
- Unsupervised Data Augmentation for Consistency Training
### Speech Recognition
- A Comparative Study on Transformer vs RNN in Speech Applications
- A deep Learning Approach to Automatic Characterisation of Rhythm in Non-native English Speech
- A pitch extraction algorithm tuned for automatic speech recognition
- A time delay neural network architecture for efficient modeling of long temporal contexts
- Active Learning for Speech Recognition: the Power of Gradients
- Active Learning for LF-MMI Trained Neural Networks in ASR
- Adaptation Methods for Non-native Speech
- Adversarial Learning of Raw Speech Features for Domain Invariant Speech Recognition
- Adversarial Multi-task Learning of Deep Neural Networks for Robust Speech Recognition
- Adversarial Training for Multilingual Acoustic Modeling
- An exploration of dropout with LSTMs
- An overview of Automatic Speech Attribute Transcription (ASAT)
- An overview of End-to-end Automatic Speech Recognition
- Audio Augmentation for Speech Recognition
- Automatic Speech Recognition for Second Language Learning: How and Why It Actually Works
- Automatic Speech Recognition of Multiple Accented Englsih Data
- ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers
- Common Voice: A Massively-Multilingual Speech Corpus
- Computer-Assisted Pronunciation Training from Pronunciation Scoring Towards Spoken Language Learning
- Conformer: Convolution-augmented Transformer for Speech Recognition
- Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks
- ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context
- Continual Learning in Automatic Speech Recognition
- Continual Learning Using Lattice-Free MMI for Speech Recognition
- Coupled Training of Sequence-to-sequence Models for Accented Speech Recognition
- CTC-Segmentation of Large Corpora for German End-to-end Speech Recognition
- Data Augmentation for Deep Neural Network Acoustic Modeling
- Data Augmentation Improves Recognition of Foreign Accented Speech
- Data Augmenting Contrastive Learning of Speech Representations in the Time Domain
- Deep Speech2: End-to-end Speech Recognition in English and Mandarin
- Domain Adversarial Training for Accented Speech Recognition
- Emformer: Efficient Memory Transformer Based Acoustic Model for Low Latency Streaming Speech Recognition
- End-to-end Accented Speech Recognition
- End-to-end Speech Recognition Using Lattice-free MMI
- English Conversational Telephone Speech Recognition by Humans and Machines
- ESPnet: End-to-end Speech Processing Toolkit
- Espresso: A Fast End-to-End Neural Speech Recognition Toolkit
- ExKaldi-RT: A Real-Time Automatic Speech Recognition Extension Toolkit of Kaldi
- Exploring Deep Learning Architectures for Automatically Grading Non-native Spontaneous Speech
- Exploring Lexicon-Free Modeling Units for End-to-End Korean and Korean-English Code-Switching Speech Recognition
- FAIRSEQ S2T: Fast Speech-to-Text Modeling with FAIRSEQ
- Generalizing RNN-Transducer to Out-Domain Audio via Sparse Self-Attention Layers
- Full-duplex Speech-to-text System for Estonian
- HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units
- Hyperparameter experiments on end-to-end automatic speech recognition
- Improved Accented Speech Recognition using Accent Embeddings and Multi-task Learning
- Improved end-of-query detection for streaming speech recognition
- Improved Noisy Student Training for Automatic Speech Recognition
- Introducing Attribute Features to Foreign Accent Recognition
- Iterative Pseudo-Labeling for Speech Recognition
- Japanese and Korean voice search
- Jasper: An End-to-End Convolutional Neural Acoustic Model
- JHU Kaldi system for Arabic MGB-3 ASR challenge using diarization, audio-transcript alignment and transfer learning
- Joint CTC-attention based end-to-end speech recognition using multi-task learning
- Language identification with suprasegmental cues: A study based on speech resynthesis
- Leveraging Native Language Information for Improved Accented Speech Recognition
- Lhotse: a speech data representation library for the modern deep learning ecosystem
- Libri-Light: A Benchmark for ASR with Limited or No Supervision
- Librispeech: An ASR Corpus Based on Public Domain Audio Books
- Light Gated Recurrent Units for Speech Recognition
- Listen, Attend and Spell
- Mask CTC: Non-Autoregressive End-to-End ASR with CTC and Mask Predict
- Memory-Efficient Training of RNN-Transducer with Sampled Softmax
- MixSpeech: Data Augmentation for Low-Resource Automatic Speech Recognition
- MLLR-based Accent Model Adaptation without Accented Data
- Montreal Forced Aligner: trainable text-speech alignment using Kaldi
- Multi-accent Speech Recognition with Hierarchical Grapheme Based Models
- Multi-dialect Speech Recognition with A Single Sequence-to-sequence Model
- Multi-task Learning for Speech Recognition: An overview
- Output-Gate Projected Gated Recurrent Unit for Speech Recognition
- Purely sequence-trained neural networks for ASR based on lattice-free MMI
- Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition
- PyChain: A Fully Parallelized PyTorch Implementation of LF-MMI for End-to-End ASR
- PyKaldi: A Python Wrapper for Kaldi
- PyKaldi2: Yet Another Speech Toolkit Based on Kaldi and Pytorch
- Quartznet: Deep Automatic Speech Recognition with 1D Time-Channel Separable Convolutions
- Recent Developments on Espnet Toolkit Boosted By Conformer
- Robust Speech Recognition via Large-Scale Weak Supervision
- Sequence Transduction with Recurrent Neural Networks
- Some Commonly Used Speech Feature Extraction Algorithms
- SpecAugment: A simple Data Augmentation Method for Automatic Speech Recognition
- Specaugment on Large Scale Datasets
- Speech Augmentation using Wavenet in Speech Recognition
- Speech Recognition of Multiple Accented English Data Using Acoustic Model Interpolation
- Speech recognition with weighted finite-state transducers
- Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition
- SpeechBrain: A General-Purpose Speech Toolkit
- SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network
- Streaming Automatic Speech Recognition with the Transformer Models
- Streaming End-to-end Speech Recognition for Mobile Devices
- Streaming Transformer Asr With Blockwise Synchronous Beam Search
- The 2020 ESPnet Update: New Features, Broadened Applications, Performance Improvements, and Future Plans
- The Kaldi Speech Recognition Toolkit
- The Pytorch-kaldi Speech Recognition Toolkit
- Towards Online End-to-end Transformer Automatic Speech Recognition
- Towards Fast and Accurate Streaming End-To-End ASR
- Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss
- Using Accent-Spercific Pronunciation Modelling for Robust Speech Recognition
- VAD-free Streaming Hybrid CTC/Attention ASR for Unsegmented Recording
- Vocal Tract Length Perturbation (VTLP) improves speech recognition
- vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations
- Wav2Letter: an End-to-End ConvNet-based Speech Recognition System
- Wav2Letter++: A Fast Open-source Speech Recognition System
- wav2vec: Unsupervised Pre-training for Speech Recognition
- wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations
- Word Beam Search: A connectionist Temporal Classification Decoding Algorithm
- 저자원 환경의 음성인식을 위한 자기 주의를 활용한 음향 모델 학습
### Speaker Recognition
- Deep Learning Methods in Speaker Recognition: A Review
- Deep speaker: an end-to-end neural speaker embedding system
- Speaker Recognition Based on Deep Learning: An Overview
- Unsupervised Domain Adaptation via Domain Adversarial Training for Speaker Recognition
### Speech Enhancement
- A Fully Convolutional Neural Network for Speech Enhancement
- Dual-Signal Transformation LSTM Network for Real-Time Noise Suppression
- Improved Speech Enhancement with the Wave-U-Net
- Real Time Speech Enhancement in the Waveform Domain
- SEGAN: Speech Enhancement Generative Adversarial Network
- Self-Attentive VAD: Context-Aware Detection of Voice from Noise
- The INTERSPEECH 2020 Deep Noise Suppression Challenge: Datasets, Subjective Testing Framework, and Challenge Resultss
### Speech Synthesis
- A review of deep learning based speech synthesis
- Adversarial Audio Synthesis
- Emotional Speech Synthesis With Rich And Granularized Control
- Natural tts synthesis by conditioning wavenet on mel spectrogram predictions
- Tacotron: Towards end-to-end speech synthesis
- Waveglow: A flow-based generative network for speech synthesis
- Wavenet: A Generative Model for Raw Audio
### Emotion Recognition
- Multimodal Emotion Recognition with High-level Speech and Text Features
- Multimodal Speech Emotion Recognition Using Audio and Text
### Natural Language Processing
- An algorithm for suffix stripping
- Attention is all you need
- BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
- Big Bird: Transformers for Longer Sequences
- Electra: Pre-training text encoders as discriminators rather than generators
- FAIRSEQ: A Fast, Extensible Toolkit for Sequence Modeling
- Language Modeling with Deep Transformers
- Language Models are Unsupervised Multitask Learners
- Longformer: The Long-Document Transformer
- Neural Machine Translation of Rare Words with Subword Units
- Recent Trends in the Use of Deep Learning Models for Grammar Error Handling
- SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing
- Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates
- Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context
### Computer vision
- Albumentations: fast and flexible image augmentations
- An image is worth 16x16 words: Transformers for image recognition at scale
- Deep residual learning for image recognition
- EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
- Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks
- Rich feature hierarchies for accurate object detection and semantic segmentation
- U-Net: Convolutional Networks for Biomedical Image Segmentation
- Very deep convolutional networks for large-scale image recognition
- You Only Look Once: Unified, Real-Time Object detection
### Reinforcement Learning
- Neural architecture search with reinforcement learning
- Playing Atari with Deep Reinforcement Learning
### Linguistics
- Accent as a Social Symbol
- Calibrating rhythm: First language and second language studies
- Control Methods Used in a Study of the Vowels
- Correlates of linguistic rhythm in the speech signal
- Durational Variability in Speech and the Rhythm Class Hypothesis
- History of ESL Pronunciation Teaching
- Intonation
- Language Discrimination by Newborns: Toward an Understanding of the Role of Rhythm
- Measures of Native and Non-Native Rhythm in a Quantity Language
- On the distinction between 'stress-timed' and 'syllable-timed' languages
- On the Historical Phonotactic of English
- Relations between language rhythm and speech rate
- Stress-timing and Syllable-timing Reanalyzed
- Sound Change And Syllable Structure in Germanic Phonology
- Speech rhythm across turn transitions in cross-cultural talk-in-interaction
- The Environment for Open-syllable Lengthening in Middle English
- The Historical Evolution of English Pronunciation
- The Original ToBI System and the Evolution of the ToBI Framework
- The Past, Present and Future of English Rhythm
- Voice Onset Time (VOT) at 50: Theoretical and practical issues in measuring voicing distinctions
- 그림의 법칙: 연쇄 밀기 입장과 연쇄 당김 입장